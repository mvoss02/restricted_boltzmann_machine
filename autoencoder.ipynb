{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "movies = pd.read_csv('data/ml-latest-small/movies.csv', sep=',', engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('data/ml-latest-small/ratings.csv', sep=',', engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratigs with the corresponding movies\n",
    "df = pd.merge(ratings, movies, how='left', on=['movieId']).sort_values(by=['timestamp'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not needed\n",
    "df = df.drop(columns=['title', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test set\n",
    "training_set = df[:int(len(df)*0.8)]\n",
    "test_set = df[int(len(df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "training_set = np.array(training_set, dtype='int32')\n",
    "test_set = np.array(test_set, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of users and movies\n",
    "unique_users = np.unique(np.concatenate([training_set[:, 0], test_set[:, 0]]))\n",
    "nb_users = len(unique_users)\n",
    "\n",
    "unqiue_movies = np.unique(np.concatenate([training_set[:, 1], test_set[:, 1]]))\n",
    "nb_movies = len(unqiue_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_user in unique_users:\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies] = id_ratings\n",
    "        new_data.append(ratings)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data, unique_movies):  \n",
    "    # Create a dictionary for fast movie ID lookup\n",
    "    movie_id_to_index = {movie_id: index for index, movie_id in enumerate(unique_movies)}\n",
    "\n",
    "    new_data = []\n",
    "    for id_user in np.unique(data[:, 0]):  # Ensure unique users\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "\n",
    "        ratings = np.zeros(nb_movies)  # Use length of unique movies\n",
    "        for movie_id, rating in zip(id_movies, id_ratings):\n",
    "            index = movie_id_to_index.get(movie_id)  # Find index using dictionary\n",
    "            if index is not None:  # Check if movie ID exists in unique_movies\n",
    "                ratings[index] = rating\n",
    "\n",
    "        new_data.append(ratings)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to matrix\n",
    "training_set = np.array(convert(training_set, unqiue_movies))\n",
    "test_set = np.array(convert(test_set, unqiue_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the neural network\n",
    "class SAE(nn.Module): # Stacked AutoEncoder\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(nb_movies, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 50)\n",
    "        self.fc4 = nn.Linear(50, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the model\n",
    "sae =SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: 0.39746998646864323\n",
      "epoch: 2  loss: 0.26873080014134604\n",
      "epoch: 3  loss: 0.22992592893011515\n",
      "epoch: 4  loss: 0.2358830463482746\n",
      "epoch: 5  loss: 0.223013292874028\n",
      "epoch: 6  loss: 0.19740250247201127\n",
      "epoch: 7  loss: 0.2077238646662665\n",
      "epoch: 8  loss: 0.1907464091764003\n",
      "epoch: 9  loss: 0.18692997141999904\n",
      "epoch: 10  loss: 0.19469668984028018\n",
      "epoch: 11  loss: 0.18653370335591765\n",
      "epoch: 12  loss: 0.18045245886976263\n",
      "epoch: 13  loss: 0.17580514243188106\n",
      "epoch: 14  loss: 0.1698338919882279\n",
      "epoch: 15  loss: 0.17436662791594357\n",
      "epoch: 16  loss: 0.1784528402658893\n",
      "epoch: 17  loss: 0.17553224647834442\n",
      "epoch: 18  loss: 0.17147097168414321\n",
      "epoch: 19  loss: 0.16724413001901312\n",
      "epoch: 20  loss: 0.16487642131976282\n",
      "epoch: 21  loss: 0.16626018375470544\n",
      "epoch: 22  loss: 0.1664031023642253\n",
      "epoch: 23  loss: 0.1673163682089325\n",
      "epoch: 24  loss: 0.16637239011982383\n",
      "epoch: 25  loss: 0.16514152058108822\n",
      "epoch: 26  loss: 0.1640755747826646\n",
      "epoch: 27  loss: 0.16295949310788296\n",
      "epoch: 28  loss: 0.1626640744719857\n",
      "epoch: 29  loss: 0.16206424173009262\n",
      "epoch: 30  loss: 0.16236616700925935\n",
      "epoch: 31  loss: 0.16194497766783225\n",
      "epoch: 32  loss: 0.1619560987247884\n",
      "epoch: 33  loss: 0.16174712702756228\n",
      "epoch: 34  loss: 0.1616045563923243\n",
      "epoch: 35  loss: 0.1613354898340999\n",
      "epoch: 36  loss: 0.1610403043183962\n",
      "epoch: 37  loss: 0.16092228812433923\n",
      "epoch: 38  loss: 0.16076872680176071\n",
      "epoch: 39  loss: 0.16073886999609627\n",
      "epoch: 40  loss: 0.16062212601241305\n",
      "epoch: 41  loss: 0.16048695050771336\n",
      "epoch: 42  loss: 0.16042345077383174\n",
      "epoch: 43  loss: 0.16023445306876966\n",
      "epoch: 44  loss: 0.1601120895311538\n",
      "epoch: 45  loss: 0.1599896943599425\n",
      "epoch: 46  loss: 0.15991132727153254\n",
      "epoch: 47  loss: 0.15989530926713125\n",
      "epoch: 48  loss: 0.15984230452105289\n",
      "epoch: 49  loss: 0.1598184166960408\n",
      "epoch: 50  loss: 0.1596780736789283\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "nb_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "    \n",
    "    for i in range(0, len(unique_users[:522]), batch_size):\n",
    "        input = Variable(training_set[i:i + batch_size]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        \n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "            s += 1.0\n",
    "            optimizer.step()\n",
    "            \n",
    "    print('epoch:', str(epoch) + '  ' + 'loss:', str(train_loss/s))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9047637107726776\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "test_loss = 0\n",
    "s = 0.0\n",
    "\n",
    "for i in range(0, len(unique_users[:522])):\n",
    "    input = Variable(training_set[i]).unsqueeze(0)\n",
    "    target = input.clone()\n",
    "    \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        s += 1.0\n",
    "            \n",
    "\n",
    "print('test loss:', str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
