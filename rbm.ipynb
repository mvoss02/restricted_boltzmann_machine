{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "movies = pd.read_csv('data/ml-latest-small/movies.csv', sep=',', engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('data/ml-latest-small/ratings.csv', sep=',', engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratigs with the corresponding movies\n",
    "df = pd.merge(ratings, movies, how='left', on=['movieId']).sort_values(by=['timestamp'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns not needed\n",
    "df = df.drop(columns=['title', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test set\n",
    "training_set = df[:int(len(df)*0.8)]\n",
    "test_set = df[int(len(df)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "training_set = np.array(training_set, dtype='int32')\n",
    "test_set = np.array(test_set, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of users and movies\n",
    "unique_users = np.unique(np.concatenate([training_set[:, 0], test_set[:, 0]]))\n",
    "nb_users = len(unique_users)\n",
    "\n",
    "unqiue_movies = np.unique(np.concatenate([training_set[:, 1], test_set[:, 1]]))\n",
    "nb_movies = len(unqiue_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_user in unique_users:\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies] = id_ratings\n",
    "        new_data.append(ratings)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data, unique_movies):  \n",
    "    # Create a dictionary for fast movie ID lookup\n",
    "    movie_id_to_index = {movie_id: index for index, movie_id in enumerate(unique_movies)}\n",
    "\n",
    "    new_data = []\n",
    "    for id_user in np.unique(data[:, 0]):  # Ensure unique users\n",
    "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
    "\n",
    "        ratings = np.zeros(nb_movies)  # Use length of unique movies\n",
    "        for movie_id, rating in zip(id_movies, id_ratings):\n",
    "            index = movie_id_to_index.get(movie_id)  # Find index using dictionary\n",
    "            if index is not None:  # Check if movie ID exists in unique_movies\n",
    "                ratings[index] = rating\n",
    "\n",
    "        new_data.append(ratings)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to matrix\n",
    "training_set = np.array(convert(training_set, unqiue_movies))\n",
    "test_set = np.array(convert(test_set, unqiue_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0., 4.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [2., 2., 2.,  ..., 0., 0., 0.],\n",
       "        [3., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ratings to binary (liked or not liked)\n",
    "training_set[training_set == 0] = -1\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "training_set[training_set >= 3] = 1\n",
    "\n",
    "test_set[test_set == 0] = -1\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "test_set[test_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
       "        [ 1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the neural network\n",
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv) * 0.1  # Weight matrix of size (nh, nv)\n",
    "        self.a = torch.zeros(1, nh)         # Bias for hidden units\n",
    "        self.b = torch.zeros(1, nv)         # Bias for visible units\n",
    "        \n",
    "    def sample_h(self, x):\n",
    "        wx = torch.mm(x, self.W.t())\n",
    "        activation = wx + self.a.expand_as(wx)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "    \n",
    "    def sample_v(self, y):\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.b.expand_as(wy)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "    \n",
    "    def train(self, v0, vk, ph0, phk, lr=0.01):\n",
    "        self.W += lr * (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
    "        self.b += lr * torch.sum((v0 - vk), dim=0)\n",
    "        self.a += lr * torch.sum((ph0 - phk), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the model\n",
    "nv = nb_movies\n",
    "nh = 128\n",
    "batch_size = 32\n",
    "rbm =RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [ 1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1  loss: tensor(0.3633)\n",
      "epoch: 2  loss: tensor(0.2952)\n",
      "epoch: 3  loss: tensor(0.2807)\n",
      "epoch: 4  loss: tensor(0.2730)\n",
      "epoch: 5  loss: tensor(0.2685)\n",
      "epoch: 6  loss: tensor(0.2668)\n",
      "epoch: 7  loss: tensor(0.2665)\n",
      "epoch: 8  loss: tensor(0.2540)\n",
      "epoch: 9  loss: tensor(0.2521)\n",
      "epoch: 10  loss: tensor(0.2514)\n",
      "epoch: 11  loss: tensor(0.2513)\n",
      "epoch: 12  loss: tensor(0.2506)\n",
      "epoch: 13  loss: tensor(0.2506)\n",
      "epoch: 14  loss: tensor(0.2473)\n",
      "epoch: 15  loss: tensor(0.2433)\n",
      "epoch: 16  loss: tensor(0.2442)\n",
      "epoch: 17  loss: tensor(0.2439)\n",
      "epoch: 18  loss: tensor(0.2388)\n",
      "epoch: 19  loss: tensor(0.2327)\n",
      "epoch: 20  loss: tensor(0.2319)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.0\n",
    "    \n",
    "    for i in range(0, len(unique_users), batch_size):\n",
    "        vk = training_set[i:i + batch_size]\n",
    "        v0 = training_set[i:i + batch_size]\n",
    "        \n",
    "        if vk.shape[0] != batch_size:\n",
    "            continue\n",
    "        \n",
    "        ph0, _ = rbm.sample_h(v0)\n",
    "        \n",
    "        for k in range(10):\n",
    "            _, hk = rbm.sample_h(vk)\n",
    "            _, vk = rbm.sample_v(hk)\n",
    "            vk[v0 < 0] = v0[v0 < 0]\n",
    "        \n",
    "        phk, _ = rbm.sample_h(vk)\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
    "        \n",
    "        s += 1.0\n",
    "        \n",
    "    print('epoch:', str(epoch) + '  ' + 'loss:', str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.2979)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the test set\n",
    "test_loss = 0\n",
    "s = 0.0\n",
    "\n",
    "for i in range(len(unique_users)):\n",
    "    v = training_set[i:i + 1]\n",
    "    vt = test_set[i:i + 1]\n",
    "    \n",
    "    if (len(vt[vt >= 0]) > 0):\n",
    "        _, h = rbm.sample_h(v)\n",
    "        _, v = rbm.sample_v(h)\n",
    "        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n",
    "        s += 1.0\n",
    "    \n",
    "\n",
    "print('loss:', str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
